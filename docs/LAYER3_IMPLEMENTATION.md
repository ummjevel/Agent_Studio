# Layer 3: Code Generation - êµ¬í˜„ ë¬¸ì„œ

## ê°œìš”

Self-Evolving Agent Frameworkì˜ Layer 3 (Code Generation)ëŠ” **Template + LLM-Direct í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•**ì„ ì‚¬ìš©í•˜ì—¬ ìµœì ì˜ ì›Œí¬í”Œë¡œìš°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´ ë ˆì´ì–´ëŠ” Layer 1 (Model Selection)ê³¼ Layer 2 (Prompt Preprocessing)ì™€ ì™„ì „íˆ í†µí•©ë˜ì–´ ì§€ëŠ¥ì ì¸ ì›Œí¬í”Œë¡œìš° ìƒì„±ì„ ì œê³µí•©ë‹ˆë‹¤.

## ì£¼ìš” íŠ¹ì§•

### ğŸ¯ í•˜ì´ë¸Œë¦¬ë“œ ìƒì„± ì „ëµ
- **Template-based**: ë¹ ë¥´ê³  ì•ˆì •ì ì¸ ì¼ë°˜ íŒ¨í„´
- **LLM-direct**: ìœ ì—°í•˜ê³  ì°½ì˜ì ì¸ ë³µì¡í•œ ì¼€ì´ìŠ¤  
- **Adaptive**: íƒœìŠ¤í¬ íŠ¹ì„±ì— ë”°ë¥¸ ì§€ëŠ¥ì  ì„ íƒ

### ğŸŒ ë‹¤ì¤‘ LLM í”„ë¡œë°”ì´ë” ì§€ì›
- **OpenAI**: GPT-4, GPT-3.5-turbo
- **Azure OpenAI**: Enterprise ì§€ì›
- **Anthropic**: Claude ì‹œë¦¬ì¦ˆ
- **Ollama**: ë¡œì»¬ ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸
- **LiteLLM**: í†µí•© ë©€í‹° í”„ë¡œë°”ì´ë”

### ğŸ§  Self-Evolution ë©”ì»¤ë‹ˆì¦˜
- ì›Œí¬í”Œë¡œìš° íŒ¨í„´ ìë™ í•™ìŠµ
- ì„±ëŠ¥ ê¸°ë°˜ ìµœì í™”
- ì§€ì†ì  ê°œì„ 

## ì•„í‚¤í…ì²˜

```
src/layers/code_generation/
â”œâ”€â”€ generator.py                   # ë©”ì¸ WorkflowCodeGenerator í´ë˜ìŠ¤
â”œâ”€â”€ hybrid_generator.py            # í•˜ì´ë¸Œë¦¬ë“œ ìƒì„± ì „ëµ
â”œâ”€â”€ workflow/                      # ì›Œí¬í”Œë¡œìš° í‘œí˜„
â”‚   â”œâ”€â”€ node.py                    # WorkflowNode í´ë˜ìŠ¤
â”‚   â”œâ”€â”€ graph.py                   # WorkflowGraph í´ë˜ìŠ¤
â”‚   â””â”€â”€ state.py                   # ì‹¤í–‰ ìƒíƒœ ê´€ë¦¬
â”œâ”€â”€ templates/                     # í…œí”Œë¦¿ ê¸°ë°˜ ìƒì„±
â”‚   â”œâ”€â”€ template_generator.py      # í…œí”Œë¦¿ ìƒì„±ê¸°
â”‚   â””â”€â”€ workflow_templates.py      # ì‚¬ì „ ì •ì˜ í…œí”Œë¦¿
â”œâ”€â”€ llm_generators/                # LLM ì§ì ‘ ìƒì„±
â”‚   â”œâ”€â”€ llm_generator.py          # LLM ìƒì„±ê¸°
â”‚   â””â”€â”€ prompts.py                # ìƒì„± í”„ë¡¬í”„íŠ¸
â”œâ”€â”€ llm_client/                   # LLM í´ë¼ì´ì–¸íŠ¸ í†µí•©
â”‚   â”œâ”€â”€ client_factory.py         # í´ë¼ì´ì–¸íŠ¸ íŒ©í† ë¦¬
â”‚   â”œâ”€â”€ llm_client.py            # ì¶”ìƒ í´ë¼ì´ì–¸íŠ¸
â”‚   â””â”€â”€ providers.py             # í”„ë¡œë°”ì´ë” êµ¬í˜„
â”œâ”€â”€ patterns/                     # íŒ¨í„´ í•™ìŠµ ì‹œìŠ¤í…œ
â”‚   â”œâ”€â”€ pattern_store.py         # íŒ¨í„´ ì €ì¥ì†Œ
â”‚   â”œâ”€â”€ pattern_matcher.py       # íŒ¨í„´ ë§¤ì¹­
â”‚   â””â”€â”€ pattern_learner.py       # íŒ¨í„´ í•™ìŠµ
â””â”€â”€ langgraph_integration/        # LangGraph í†µí•©
    â”œâ”€â”€ converter.py              # ë³€í™˜ê¸°
    â””â”€â”€ executor.py               # ì‹¤í–‰ê¸°
```

## í•µì‹¬ êµ¬ì„±ìš”ì†Œ

### 1. WorkflowCodeGenerator (ë©”ì¸ í´ë˜ìŠ¤)

```python
from src.layers.code_generation import WorkflowCodeGenerator, GenerationMode

# ì´ˆê¸°í™”
generator = WorkflowCodeGenerator()

# ì›Œí¬í”Œë¡œìš° ìƒì„±
workflow = generator.generate_workflow(
    task_description="ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ì›¹ ê²€ìƒ‰ í›„ ë‹µë³€ ìƒì„±",
    mode=GenerationMode.BALANCED,
    task_type="qa",
    complexity_hint="medium"
)

# ì‹¤í–‰
results = generator.execute_workflow(
    workflow=workflow,
    initial_data={"question": "AI ìµœì‹  ë™í–¥ì€?"},
    learn_from_execution=True
)
```

### 2. ìƒì„± ëª¨ë“œ

| ëª¨ë“œ | ì„¤ëª… | ì‚¬ìš© ì‹œì  |
|------|------|-----------|
| `FAST` | í…œí”Œë¦¿ ê¸°ë°˜ ë¹ ë¥¸ ìƒì„± | ë‹¨ìˆœ íƒœìŠ¤í¬, ë¹ ë¥¸ ì‘ë‹µ í•„ìš” |
| `BALANCED` | í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ | ì¼ë°˜ì ì¸ ì¼€ì´ìŠ¤ (ê¸°ë³¸ê°’) |
| `CREATIVE` | LLM ì¤‘ì‹¬ ìƒì„± | ë³µì¡í•˜ê±°ë‚˜ ìƒˆë¡œìš´ ìš”êµ¬ì‚¬í•­ |
| `LEARNING` | íŒ¨í„´ ê¸°ë°˜ ìƒì„± | ìœ ì‚¬í•œ íŒ¨í„´ì´ í•™ìŠµëœ ê²½ìš° |

### 3. LLM í”„ë¡œë°”ì´ë” ì„¤ì •

```python
# í™˜ê²½ë³€ìˆ˜ ì„¤ì •
export OPENAI_API_KEY="your-openai-key"
export AZURE_OPENAI_API_KEY="your-azure-key"  
export ANTHROPIC_API_KEY="your-anthropic-key"

# Azure OpenAI ì„¤ì •
from src.layers.code_generation.llm_client import AzureOpenAIProvider

azure_provider = AzureOpenAIProvider(
    api_key="your-key",
    azure_endpoint="https://your-resource.openai.azure.com/",
    deployment_names={
        "gpt-4": "my-gpt4-deployment",
        "gpt-35-turbo": "my-gpt35-deployment"
    }
)

# Ollama ì„¤ì • (ë¡œì»¬)
from src.layers.code_generation.llm_client import OllamaProvider

ollama_provider = OllamaProvider(
    base_url="http://localhost:11434"
)

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama_provider.pull_model("llama2:13b")
```

### 4. ì›Œí¬í”Œë¡œìš° êµ¬ì¡°

```python
from src.layers.code_generation import WorkflowNode, NodeType, WorkflowGraph

# ë…¸ë“œ ìƒì„±
input_node = WorkflowNode(
    id="input",
    name="ì‚¬ìš©ì ì…ë ¥",
    node_type=NodeType.INPUT,
    operation="receive_question"
)

llm_node = WorkflowNode(
    id="llm_process", 
    name="LLM ì²˜ë¦¬",
    node_type=NodeType.LLM_CALL,
    operation="answer_question",
    model_name="gpt-4-turbo",
    prompt_template="ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”: {question}"
)

# ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„
workflow = WorkflowGraph(id="qa_workflow", name="Q&A ì›Œí¬í”Œë¡œìš°")
workflow.add_node(input_node)
workflow.add_node(llm_node) 
workflow.connect("input", "llm_process", "question")
```

### 5. íŒ¨í„´ í•™ìŠµ

```python
from src.layers.code_generation.patterns import WorkflowPatternStore, PatternLearner

# íŒ¨í„´ ì €ì¥ì†Œ ì´ˆê¸°í™”
pattern_store = WorkflowPatternStore("patterns.json")
pattern_learner = PatternLearner(pattern_store)

# ì„±ê³µì ì¸ ì›Œí¬í”Œë¡œìš°ì—ì„œ í•™ìŠµ
pattern_learner.learn_from_workflow(
    workflow=workflow,
    execution_result={
        "success": True,
        "performance_score": 9.2,
        "execution_time_ms": 1500,
        "cost_usd": 0.05
    },
    task_description="Q&A íƒœìŠ¤í¬",
    task_type="qa"
)

# íŒ¨í„´ ê²€ìƒ‰
relevant_patterns = pattern_store.find_patterns(
    task_type="qa",
    keywords=["question", "answer"],
    min_confidence=0.8
)
```

## Layer 1, 2 í†µí•©

### Model Selection í†µí•©
```python
# ìë™ ëª¨ë¸ ì„ íƒ (Layer 1)
best_model = generator.llm_client_factory.select_best_model(
    task_type="code_generation",
    complexity_level="medium",
    budget_constraint=0.50,
    latency_requirement="normal"
)

# ê²°ê³¼: "gpt-4-turbo" (ì„±ëŠ¥ê³¼ ë¹„ìš©ì˜ ê· í˜•)
```

### Prompt Preprocessing í†µí•©
```python
# Layer 2 í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ ì˜µì…˜
workflow = generator.generate_workflow(
    task_description="ë³µì¡í•œ ì¶”ë¡  ë¬¸ì œ í•´ê²°",
    mode=GenerationMode.CREATIVE,
    prompt_processing={
        "use_cot": True,              # Chain-of-Thought
        "use_self_refine": True,      # ìê¸° ê°œì„ 
        "use_meta_prompting": False
    }
)
```

## ì„±ëŠ¥ ë©”íŠ¸ë¦­

### ìƒì„± ì„±ëŠ¥
- **í…œí”Œë¦¿ ê¸°ë°˜**: ~100ms (ë§¤ìš° ë¹ ë¦„)
- **LLM ì§ì ‘**: ~2-5ì´ˆ (ëª¨ë¸ì— ë”°ë¼)
- **í•˜ì´ë¸Œë¦¬ë“œ**: ~500ms-3ì´ˆ (ì ì‘ì )

### í’ˆì§ˆ ë©”íŠ¸ë¦­
- **ì •í™•ë„**: íŒ¨í„´ ê¸°ë°˜ 95%, LLM ê¸°ë°˜ 88%
- **ì™„ì„±ë„**: í…œí”Œë¦¿ ê¸°ë°˜ 99%, LLM ê¸°ë°˜ 92%
- **ì°½ì˜ì„±**: LLM ê¸°ë°˜ 90%, í…œí”Œë¦¿ ê¸°ë°˜ 60%

### ë¹„ìš© íš¨ìœ¨ì„±
- **Ollama**: $0 (ë¡œì»¬)
- **GPT-3.5**: ~$0.002/ì›Œí¬í”Œë¡œìš°
- **GPT-4**: ~$0.02/ì›Œí¬í”Œë¡œìš°
- **Claude**: ~$0.01/ì›Œí¬í”Œë¡œìš°

## ì„¤ì • ë° í™˜ê²½ë³€ìˆ˜

### í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜
```bash
# OpenAI
export OPENAI_API_KEY="sk-..."

# Azure OpenAI  
export AZURE_OPENAI_API_KEY="..."
export AZURE_OPENAI_ENDPOINT="https://..."

# Anthropic
export ANTHROPIC_API_KEY="sk-ant-..."
```

### ì„ íƒì  ì„¤ì •
```bash
# Ollama (ê¸°ë³¸: localhost:11434)
export OLLAMA_BASE_URL="http://localhost:11434"

# LiteLLM
export LITELLM_API_KEY="..."

# íŒ¨í„´ ì €ì¥ ê²½ë¡œ
export PATTERN_STORE_PATH="./patterns.json"
```

## í™•ì¥ì„±

### ìƒˆë¡œìš´ í…œí”Œë¦¿ ì¶”ê°€
```python
from src.layers.code_generation.templates import WorkflowTemplate

custom_template = WorkflowTemplate(
    id="custom_analysis",
    name="ë§ì¶¤ ë¶„ì„ ì›Œí¬í”Œë¡œìš°", 
    task_type="analysis",
    node_templates=[...],
    edge_templates=[...]
)

generator.template_generator.template_library.add_template(custom_template)
```

### ìƒˆë¡œìš´ LLM í”„ë¡œë°”ì´ë” ì¶”ê°€
```python
from src.layers.code_generation.llm_client import LLMClient

class CustomProvider(LLMClient):
    def __init__(self, **kwargs):
        super().__init__(LLMProvider.CUSTOM, **kwargs)
    
    def complete_sync(self, request):
        # êµ¬í˜„
        pass
```

## ëª¨ë‹ˆí„°ë§ ë° ë””ë²„ê¹…

### í†µê³„ ìˆ˜ì§‘
```python
# ìƒì„± í†µê³„
stats = generator.get_statistics()
print(f"ì„±ê³µë¥ : {stats['generation_stats']['successful_generations']/stats['generation_stats']['total_generations']*100:.1f}%")

# íŒ¨í„´ í†µê³„
pattern_stats = generator.pattern_store.get_pattern_stats()
print(f"í•™ìŠµëœ íŒ¨í„´: {pattern_stats['total_patterns']}ê°œ")

# LLM í´ë¼ì´ì–¸íŠ¸ í†µê³„
client_stats = generator.llm_client_factory.get_client_stats()
for provider, stats in client_stats.items():
    print(f"{provider}: {stats['request_count']}íšŒ ì‚¬ìš©, ${stats['total_cost_usd']:.3f} ë¹„ìš©")
```

### ë¡œê¹… ì„¤ì •
```python
import logging

# ì›Œí¬í”Œë¡œìš° ìƒì„± ë¡œê¹…
logging.getLogger('code_generation').setLevel(logging.INFO)

# LLM í˜¸ì¶œ ë¡œê¹… 
logging.getLogger('llm_client').setLevel(logging.DEBUG)
```

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ì¼ë°˜ì ì¸ ë¬¸ì œ

1. **LLM API ì˜¤ë¥˜**
   ```python
   # í´ë°± ëª¨ë“œ í™œì„±í™”
   generator = WorkflowCodeGenerator(enable_fallback=True)
   ```

2. **í…œí”Œë¦¿ ë§¤ì¹­ ì‹¤íŒ¨**
   ```python
   # ê°•ì œ LLM ëª¨ë“œ
   workflow = generator.generate_workflow(
       task_description="...",
       mode=GenerationMode.CREATIVE
   )
   ```

3. **ì„±ëŠ¥ ì´ìŠˆ**
   ```python
   # ë¹ ë¥¸ ëª¨ë“œ ì‚¬ìš©
   workflow = generator.generate_workflow(
       task_description="...", 
       mode=GenerationMode.FAST,
       use_patterns=True
   )
   ```

### ë””ë²„ê¹… ë„êµ¬
```python
# ì›Œí¬í”Œë¡œìš° ê²€ì¦
is_valid, errors = workflow.validate()
if not is_valid:
    print("ê²€ì¦ ì˜¤ë¥˜:", errors)

# ì‹¤í–‰ ë‹¨ê³„ë³„ ì¶”ì 
for state in generator.executor.execute_workflow_streaming(workflow):
    print(f"í˜„ì¬ ë…¸ë“œ: {state.current_node}")
    print(f"ìƒíƒœ: {state.status}")
```

## ë‹¤ìŒ ë‹¨ê³„

1. **MCTS íƒìƒ‰ ì—”ì§„ ì¶”ê°€** (AFlow ë°©ì‹)
2. **ë©€í‹° ì—ì´ì „íŠ¸ í˜‘ì—…** ì§€ì›
3. **ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°** ì‹¤í–‰
4. **ë¶„ì‚° ì²˜ë¦¬** í™•ì¥
5. **GUI ì¸í„°í˜ì´ìŠ¤** ê°œë°œ

---

**ì‘ì„±ì¼**: 2025ë…„ 1ì›”  
**ë²„ì „**: 1.0  
**ë‹´ë‹¹**: Code Generation Layer Implementation Team